{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1:- Mount Google Drive to access files stored in your Drive (e.g., datasets)"
      ],
      "metadata": {
        "id": "OA10ju7AVLKz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7xQy3ZSoS42",
        "outputId": "dd804946-57fa-40fe-db92-a61b30cdb0f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2:- Import nltk library and download 'wordnet' corpus needed for lemmatization"
      ],
      "metadata": {
        "id": "vID-PHpPVNcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oARQZU1wpEZ3",
        "outputId": "1d74cc31-234d-4a81-b858-1f9a27f79685"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3:- Install required Python packages using pip"
      ],
      "metadata": {
        "id": "lOMe6zN2VXhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "!pip install streamlit pandas scikit-learn nltk openpyxl\n",
        "!pip install fuzzywuzzy python-Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plmPPxSmocfy",
        "outputId": "098568dc-5f68-4851-818f-990bfef2f660"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.41.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.9)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.41.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.11/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.11/dist-packages (0.27.1)\n",
            "Requirement already satisfied: Levenshtein==0.27.1 in /usr/local/lib/python3.11/dist-packages (from python-Levenshtein) (0.27.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from Levenshtein==0.27.1->python-Levenshtein) (3.13.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4:- Fetch and print your public IP address (optional, useful to check network)"
      ],
      "metadata": {
        "id": "rMBtSOdeVpwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGzlQzAoohd9",
        "outputId": "15e9703b-6f75-43f5-e648-97014b39b681"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.148.57.207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5:- Run the app.py"
      ],
      "metadata": {
        "id": "BG0daGsTV0y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import re\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load dataset\n",
        "@st.cache_data\n",
        "def load_dataset(filepath):\n",
        "    df = pd.read_excel(filepath)\n",
        "    return df\n",
        "\n",
        "# Preprocess dataset\n",
        "def preprocess_data(df):\n",
        "    X = df['Slang/Idiom'].astype(str)\n",
        "    y = df['Category'].astype(str)\n",
        "    return X, y\n",
        "\n",
        "# Train classification model\n",
        "@st.cache_data(show_spinner=False)\n",
        "def train_model(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=5000)\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = vectorizer.transform(X_test)\n",
        "    model = RandomForestClassifier(random_state=42, class_weight=\"balanced\")\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "    return model, vectorizer, X_test_tfidf, y_test\n",
        "\n",
        "# General Marathi stopwords/filler words to ignore\n",
        "stopwords = {\n",
        "    \"‡§Æ‡§ó\", \"‡§Ö‡§∞‡•á\", \"‡§π‡•ã\", \"‡§ï‡§æ\", \"‡§Ü‡§π‡•á\", \"‡§®‡§æ‡§π‡•Ä\", \"‡§Æ‡•Ä\", \"‡§§‡•Ç\", \"‡§§‡•ã\", \"‡§§‡•Ä\", \"‡§Ü‡§£‡§ø\",\n",
        "    \"‡§™‡§£\", \"‡§§‡§∞\", \"‡§π‡•Ä\", \"‡§§‡•ã‡§ö\", \"‡§ï‡§ø‡§Ç‡§µ‡§æ\", \"‡§ï‡•Å‡§†‡•á\", \"‡§ï‡§ß‡•Ä\", \"‡§ï‡§∂‡§æ‡§≤‡§æ\", \"‡§ï‡§∏‡§æ\", \"‡§ï‡§∂‡•Ä\"\n",
        "}\n",
        "\n",
        "# Function to ignore non-slang/idiom phrases\n",
        "def is_non_slang_phrase(phrase):\n",
        "    if len(phrase.split()) <= 2:\n",
        "        return True\n",
        "    words = phrase.split()\n",
        "    if all(word in stopwords for word in words):\n",
        "        return True\n",
        "    question_words = {\"‡§ï‡§æ\", \"‡§ï‡§ß‡•Ä\", \"‡§ï‡•Å‡§†‡•á\", \"‡§ï‡§∂‡§æ‡§≤‡§æ\", \"‡§ï‡§∏‡§æ\", \"‡§ï‡§∂‡•Ä\"}\n",
        "    interjections = {\"‡§Ö‡§∞‡•á\", \"‡§π‡•á\", \"‡§ì\", \"‡§Ö‡§π‡•ã\"}\n",
        "    first_word = words[0]\n",
        "    if first_word in question_words or first_word in interjections:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Classify slangs and idioms\n",
        "def classify_slangs_and_idioms(text, model, vectorizer, slang_df, confidence_threshold=0.6):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sentences = re.split(r'[.?!,;\"‚Äú‚Äù]+', text)\n",
        "    all_detected = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        if not sentence:\n",
        "            continue\n",
        "        if is_non_slang_phrase(sentence):\n",
        "            continue\n",
        "\n",
        "        lemmatized_sentence = ' '.join([lemmatizer.lemmatize(word) for word in sentence.split()])\n",
        "        sentence_tfidf = vectorizer.transform([lemmatized_sentence])\n",
        "\n",
        "        proba = model.predict_proba(sentence_tfidf)[0]\n",
        "        max_confidence = max(proba)\n",
        "        prediction = model.classes_[proba.argmax()]\n",
        "\n",
        "        if max_confidence < confidence_threshold:\n",
        "            continue\n",
        "\n",
        "        # Check against dataset slangs/idioms\n",
        "        for slang_idiom, meaning in zip(slang_df['Slang/Idiom'], slang_df['Meaning']):\n",
        "            lemmatized_slang = ' '.join([lemmatizer.lemmatize(word) for word in slang_idiom.split()])\n",
        "            if lemmatized_slang in lemmatized_sentence and slang_idiom not in all_detected:\n",
        "                all_detected[slang_idiom] = {\n",
        "                    'type': prediction,\n",
        "                    'slang_idiom': slang_idiom,\n",
        "                    'meaning': meaning\n",
        "                }\n",
        "\n",
        "        # Check manual dictionary with fuzzy matching (>80% partial ratio)\n",
        "        for manual_key, manual_value in manual_slang_idioms.items():\n",
        "            if fuzz.partial_ratio(manual_key, sentence) > 80 and manual_key not in all_detected:\n",
        "                all_detected[manual_key] = {\n",
        "                    'type': manual_value['type'],\n",
        "                    'slang_idiom': manual_key,\n",
        "                    'meaning': manual_value['meaning']\n",
        "                }\n",
        "\n",
        "    return list(all_detected.values())\n",
        "\n",
        "# Evaluate model\n",
        "def evaluate_model(model, X_test_tfidf, y_test):\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    return accuracy, precision, recall, conf_matrix\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"Marathi Slang/Idiom Detector\", layout=\"wide\", page_icon=\"üîç\")\n",
        "\n",
        "    # Custom CSS Styling\n",
        "    st.markdown(\"\"\"\n",
        "        <style>\n",
        "            body {\n",
        "                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "                background-color: #f5f7fa;\n",
        "            }\n",
        "            .stButton>button {\n",
        "                background-color: #00adb5;\n",
        "                color: white;\n",
        "                font-weight: bold;\n",
        "                border-radius: 8px;\n",
        "                height: 3em;\n",
        "                width: 100%;\n",
        "                font-size: 16px;\n",
        "                margin-top: 10px;\n",
        "            }\n",
        "            .stTextArea>div>textarea {\n",
        "                border-radius: 10px;\n",
        "                border: 1px solid #ccc;\n",
        "                font-size: 16px;\n",
        "                padding: 12px;\n",
        "            }\n",
        "            .block-container {\n",
        "                padding: 2rem 3rem;\n",
        "            }\n",
        "            h1, h2, h3 {\n",
        "                color: #007bff;\n",
        "            }\n",
        "            footer {visibility: hidden;}\n",
        "        </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    # Sidebar\n",
        "    with st.sidebar:\n",
        "        st.markdown(\"<h2 style='color: #007bff;'>Marathi Slang & Idiom Detection App</h2>\", unsafe_allow_html=True)\n",
        "        st.write(\"üöÄ Detects Marathi slangs and idioms and provides their meanings.\")\n",
        "        st.markdown(\"---\")\n",
        "        st.header(\"Instructions\")\n",
        "        st.write(\n",
        "            \"1. Enter a Marathi paragraph.\\n\"\n",
        "            \"2. Click **Classify** to detect slangs/idioms.\\n\"\n",
        "            \"3. Click **Evaluate Model** to see model performance.\"\n",
        "        )\n",
        "\n",
        "    # Main Title and Description\n",
        "    st.markdown(\"<h1 style='text-align: center;'>Marathi Slang/Idiom Detection üîç</h1>\", unsafe_allow_html=True)\n",
        "    st.write(\"This app identifies Marathi slangs and idioms in your text and provides their meanings if available.\")\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # Load dataset and train model once\n",
        "    filepath = '/content/drive/MyDrive/NLP Mini Project Dataset/Marathi Slang And Idiom.xlsx'\n",
        "    slang_df = load_dataset(filepath)\n",
        "    X, y = preprocess_data(slang_df)\n",
        "    model, vectorizer, X_test_tfidf, y_test = train_model(X, y)\n",
        "\n",
        "    # Input and buttons in columns for neat layout\n",
        "    col1, col2 = st.columns([3, 1])\n",
        "\n",
        "    with col1:\n",
        "        st.markdown(\"<h3>Enter a Marathi Paragraph:</h3>\", unsafe_allow_html=True)\n",
        "        user_input = st.text_area(\"Paste your Marathi text here:\")\n",
        "\n",
        "    with col2:\n",
        "        st.markdown(\"<h3>Actions</h3>\", unsafe_allow_html=True)\n",
        "        classify_clicked = st.button(\"üîç Classify\")\n",
        "        evaluate_clicked = st.button(\"üìà Evaluate Model\")\n",
        "\n",
        "    # Handle classification\n",
        "    if classify_clicked:\n",
        "        if user_input and user_input.strip():\n",
        "            detected_items = classify_slangs_and_idioms(user_input, model, vectorizer, slang_df)\n",
        "            if detected_items:\n",
        "                st.markdown(\"### Recognized Slangs/Idioms:\")\n",
        "                for item in detected_items:\n",
        "                    st.success(f\"‚úÖ **{item['type'].capitalize()}** ‚Üí *{item['slang_idiom']}* ‚û°Ô∏è {item['meaning']}\")\n",
        "            else:\n",
        "                st.warning(\"‚ö†Ô∏è No slangs or idioms identified.\")\n",
        "        else:\n",
        "            st.error(\"‚ö†Ô∏è Please enter valid text.\")\n",
        "\n",
        "    # Handle evaluation\n",
        "    if evaluate_clicked:\n",
        "        accuracy, precision, recall, conf_matrix = evaluate_model(model, X_test_tfidf, y_test)\n",
        "        st.markdown(\"### Model Performance Metrics üìä\")\n",
        "        st.metric(label=\"Accuracy\", value=f\"{accuracy * 100:.2f}%\")\n",
        "        st.metric(label=\"Precision\", value=f\"{precision:.2f}\")\n",
        "        st.metric(label=\"Recall\", value=f\"{recall:.2f}\")\n",
        "\n",
        "        # Plot confusion matrix heatmap\n",
        "        st.markdown(\"### Confusion Matrix\")\n",
        "        fig, ax = plt.subplots(figsize=(4, 3))\n",
        "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "        ax.set_xlabel(\"Predicted\")\n",
        "        ax.set_ylabel(\"Actual\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpYnvHlHol_C",
        "outputId": "2ec89856-9444-43d9-8821-16ea52703c94"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6:- Run Streamlit app in background and expose it publicly using localtunnel on port 8501"
      ],
      "metadata": {
        "id": "X4Ma4iCDVvEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVagkVS-ppFU",
        "outputId": "967bfc57-0ad4-491f-de70-2cdd80f27e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.148.57.207:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0Kyour url is: https://cute-knives-show.loca.lt\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}